{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import copy\n",
    "from facenet_pytorch import InceptionResnetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "plt.ion()   # interactive mode\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(299),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "data_dir = 'data'\n",
    "image_datasets = datasets.ImageFolder(data_dir,data_transforms)\n",
    "\n",
    "dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
    "dataset_sizes = len(image_datasets) \n",
    "\n",
    "class_names = image_datasets.classes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using \" + str(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = dict(Counter(image_datasets.targets))\n",
    "\n",
    "names = list(image_datasets.classes)\n",
    "values = list(data.values())\n",
    "\n",
    "plt.bar(range(len(data)), values, tick_label=names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 299, 299])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloaders))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp  = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    inp  = std * inp + mean\n",
    "    inp  = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(5)  # pause a bit so that plots are updated\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        for inputs, labels in dataloaders:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            loss = loss.item() # detach gradient\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1) \n",
    "            corrects = torch.sum(preds == labels)\n",
    "\n",
    "            running_accuracy += corrects\n",
    "            running_loss += loss\n",
    "\n",
    "        running_loss /= dataset_sizes\n",
    "        running_accuracy /= dataset_sizes\n",
    "        print(f\"loss = {running_loss} accuracy = {running_accuracy}\")\n",
    "\n",
    "        if running_accuracy > best_acc:\n",
    "            best_acc = running_accuracy\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "loss = 0.0408822232176402 accuracy = 0.8008999228477478\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "loss = 0.03608253028315017 accuracy = 0.8436445593833923\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "loss = 0.035795061066495285 accuracy = 0.8503937125205994\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "loss = 0.035351493126629975 accuracy = 0.84139484167099\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "loss = 0.03375145395708567 accuracy = 0.8492688536643982\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "loss = 0.0362340750924916 accuracy = 0.8503937125205994\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "loss = 0.03502310518305267 accuracy = 0.854893147945404\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "loss = 0.03316835173237042 accuracy = 0.8503937125205994\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "loss = 0.03372412690362726 accuracy = 0.8560180068016052\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "loss = 0.03251412919857639 accuracy = 0.8672666549682617\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "loss = 0.03273484551665217 accuracy = 0.8571428656578064\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "loss = 0.03199609657340967 accuracy = 0.8458942770957947\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "loss = 0.03428360239861384 accuracy = 0.8537682890892029\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "loss = 0.03423489483976391 accuracy = 0.8582677245140076\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "loss = 0.03159386911026136 accuracy = 0.8616423010826111\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "loss = 0.03917096541823633 accuracy = 0.834645688533783\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "loss = 0.03305030951886129 accuracy = 0.8515185713768005\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "loss = 0.033978797889265354 accuracy = 0.8560180068016052\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "loss = 0.03481872758594487 accuracy = 0.8515185713768005\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "loss = 0.04117318782515413 accuracy = 0.8436445593833923\n",
      "\n",
      "Best val Acc: 0.867267\n",
      "Saving model trained_model.pt\n"
     ]
    }
   ],
   "source": [
    "model_ft = InceptionResnetV1(pretrained='vggface2', device=device, classify= True, num_classes=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=20)\n",
    "\n",
    "model_path = \"trained_model.pt\"\n",
    "print(\"Saving model \"+model_path)\n",
    "torch.save(model_ft.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "negatives\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "negatives\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "negatives\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "negatives\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m pil_image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m frame \u001b[39m=\u001b[39m data_transforms(pil_image)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m res \u001b[39m=\u001b[39m model_ft(frame)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(res\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(class_names[m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/facenet_pytorch/models/inception_resnet_v1.py:292\u001b[0m, in \u001b[0;36mInceptionResnetV1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    290\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepeat_2(x)\n\u001b[1;32m    291\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmixed_7a(x)\n\u001b[0;32m--> 292\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepeat_3(x)\n\u001b[1;32m    293\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock8(x)\n\u001b[1;32m    294\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool_1a(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/facenet_pytorch/models/inception_resnet_v1.py:120\u001b[0m, in \u001b[0;36mBlock8.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    119\u001b[0m     x0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbranch0(x)\n\u001b[0;32m--> 120\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbranch1(x)\n\u001b[1;32m    121\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x0, x1), \u001b[39m1\u001b[39m)\n\u001b[1;32m    122\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2d(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/facenet_pytorch/models/inception_resnet_v1.py:31\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     30\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(x)\n\u001b[0;32m---> 31\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x)\n\u001b[1;32m     32\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "checkpoint = torch.load('trained_model.pt')\n",
    "model_ft.load_state_dict(checkpoint)\n",
    "\n",
    "vc = cv2.VideoCapture(0)\n",
    "model_ft.eval()\n",
    "while True:\n",
    "    ret, frame = vc.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading frame from video capture.\")\n",
    "        break\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    frame = data_transforms(pil_image).unsqueeze(0).to(device)\n",
    "    res = model_ft(frame)\n",
    "    m = torch.argmax(res.detach())\n",
    "    print(class_names[m])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
