{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import copy\n",
    "from facenet_pytorch import InceptionResnetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "plt.ion()   # interactive mode\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(299),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "data_dir = 'data'\n",
    "image_datasets = datasets.ImageFolder(data_dir,data_transforms)\n",
    "\n",
    "dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=4, shuffle=True, num_workers=4)\n",
    "dataset_sizes = len(image_datasets) \n",
    "\n",
    "class_names = image_datasets.classes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using \" + str(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = dict(Counter(image_datasets.targets))\n",
    "\n",
    "names = list(image_datasets.classes)\n",
    "values = list(data.values())\n",
    "\n",
    "plt.bar(range(len(data)), values, tick_label=names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp  = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    inp  = std * inp + mean\n",
    "    inp  = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(5)  # pause a bit so that plots are updated\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        for inputs, labels in dataloaders:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            loss = loss.item() # detach gradient\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1) \n",
    "            corrects = torch.sum(preds == labels)\n",
    "\n",
    "            running_accuracy += corrects\n",
    "            running_loss += loss\n",
    "\n",
    "        running_loss /= dataset_sizes\n",
    "        running_accuracy /= dataset_sizes\n",
    "        print(f\"loss = {running_loss} accuracy = {running_accuracy}\")\n",
    "\n",
    "        if running_accuracy > best_acc:\n",
    "            best_acc = running_accuracy\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/39\n",
      "----------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 512])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb Cell 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m optimizer_ft \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model_ft\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m exp_lr_scheduler \u001b[39m=\u001b[39m lr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer_ft, step_size\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model_ft \u001b[39m=\u001b[39m train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                        num_epochs\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrained_model.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSaving model \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mmodel_path)\n",
      "\u001b[1;32m/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/facenet_pytorch/models/inception_resnet_v1.py:297\u001b[0m, in \u001b[0;36mInceptionResnetV1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    295\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[1;32m    296\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_linear(x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m--> 297\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_bn(x)\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassify:\n\u001b[1;32m    299\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2436\u001b[0m         batch_norm,\n\u001b[1;32m   2437\u001b[0m         (\u001b[39minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m-> 2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize())\n\u001b[1;32m   2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     size_prods \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m size[i \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m   2415\u001b[0m \u001b[39mif\u001b[39;00m size_prods \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 512])"
     ]
    }
   ],
   "source": [
    "model_ft = InceptionResnetV1(pretrained='vggface2', device=device, classify= True, num_classes=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=40)\n",
    "\n",
    "model_path = \"trained_model.pt\"\n",
    "print(\"Saving model \"+model_path)\n",
    "torch.save(model_ft.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
