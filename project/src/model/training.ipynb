{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import copy\n",
    "from facenet_pytorch import InceptionResnetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "plt.ion()   # interactive mode\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(299),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "data_dir = 'data'\n",
    "image_datasets = datasets.ImageFolder(data_dir,data_transforms)\n",
    "\n",
    "dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
    "dataset_sizes = len(image_datasets) \n",
    "\n",
    "class_names = image_datasets.classes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using \" + str(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = dict(Counter(image_datasets.targets))\n",
    "\n",
    "names = list(image_datasets.classes)\n",
    "values = list(data.values())\n",
    "\n",
    "plt.bar(range(len(data)), values, tick_label=names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 299, 299])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloaders))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp  = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    inp  = std * inp + mean\n",
    "    inp  = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(5)  # pause a bit so that plots are updated\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        for inputs, labels in dataloaders:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            loss = loss.item() # detach gradient\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1) \n",
    "            corrects = torch.sum(preds == labels)\n",
    "\n",
    "            running_accuracy += corrects\n",
    "            running_loss += loss\n",
    "\n",
    "        running_loss /= dataset_sizes\n",
    "        running_accuracy /= dataset_sizes\n",
    "        print(f\"loss = {running_loss} accuracy = {running_accuracy}\")\n",
    "\n",
    "        if running_accuracy > best_acc:\n",
    "            best_acc = running_accuracy\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "loss = 0.034377026012328664 accuracy = 0.8238189220428467\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "loss = 0.03106031605724509 accuracy = 0.8257874250411987\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "loss = 0.03161642354304396 accuracy = 0.834645688533783\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "loss = 0.028643143964623374 accuracy = 0.8415354490280151\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "loss = 0.029564368311227777 accuracy = 0.8484252095222473\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "loss = 0.030841098288000805 accuracy = 0.8277559280395508\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "loss = 0.03117874833777195 accuracy = 0.8307086825370789\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "loss = 0.029536659227992135 accuracy = 0.834645688533783\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "loss = 0.030129855469338537 accuracy = 0.8405511975288391\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "loss = 0.03237819949560982 accuracy = 0.8336614370346069\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "loss = 0.02825755530630979 accuracy = 0.8444882035255432\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "loss = 0.030315446447375723 accuracy = 0.8316929340362549\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "loss = 0.030424031406993002 accuracy = 0.8248031735420227\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "loss = 0.033318003038252435 accuracy = 0.8159448504447937\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "loss = 0.030062921623897364 accuracy = 0.837598443031311\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "loss = 0.032345065993704195 accuracy = 0.8435039520263672\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "loss = 0.029191238761652173 accuracy = 0.836614191532135\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "loss = 0.03117376260870085 accuracy = 0.835629940032959\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "loss = 0.028131489451299972 accuracy = 0.8435039520263672\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "loss = 0.030708658948951348 accuracy = 0.8405511975288391\n",
      "\n",
      "Best val Acc: 0.848425\n",
      "Saving model trained_model.pt\n"
     ]
    }
   ],
   "source": [
    "model_ft = InceptionResnetV1(pretrained='vggface2', device=device, classify= True, num_classes=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=20)\n",
    "\n",
    "model_path = \"trained_model.pt\"\n",
    "print(\"Saving model \"+model_path)\n",
    "torch.save(model_ft.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "negatives\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "negatives\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "negatives\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "negatives\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "klara_positive\n",
      "klara_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "negatives\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n",
      "daniele_positive\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m pil_image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m frame \u001b[39m=\u001b[39m data_transforms(pil_image)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m res \u001b[39m=\u001b[39m model_ft(frame)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(res\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniaffch/Desktop/Uni/Human_Robot/hcir-assignments/project/src/model/training.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(class_names[m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/facenet_pytorch/models/inception_resnet_v1.py:292\u001b[0m, in \u001b[0;36mInceptionResnetV1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    290\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepeat_2(x)\n\u001b[1;32m    291\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmixed_7a(x)\n\u001b[0;32m--> 292\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepeat_3(x)\n\u001b[1;32m    293\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock8(x)\n\u001b[1;32m    294\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool_1a(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/facenet_pytorch/models/inception_resnet_v1.py:120\u001b[0m, in \u001b[0;36mBlock8.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    119\u001b[0m     x0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbranch0(x)\n\u001b[0;32m--> 120\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbranch1(x)\n\u001b[1;32m    121\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x0, x1), \u001b[39m1\u001b[39m)\n\u001b[1;32m    122\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2d(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/facenet_pytorch/models/inception_resnet_v1.py:31\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     30\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(x)\n\u001b[0;32m---> 31\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x)\n\u001b[1;32m     32\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "checkpoint = torch.load('trained_model.pt')\n",
    "model_ft.load_state_dict(checkpoint)\n",
    "\n",
    "vc = cv2.VideoCapture(0)\n",
    "model_ft.eval()\n",
    "while True:\n",
    "    ret, frame = vc.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading frame from video capture.\")\n",
    "        break\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    frame = data_transforms(pil_image).unsqueeze(0).to(device)\n",
    "    res = model_ft(frame)\n",
    "    m = torch.argmax(res.detach())\n",
    "    print(class_names[m])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
